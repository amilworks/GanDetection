{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **AmilGan Detection using NASNET**\n",
    "#### Amil Khan | March 1, 2019 | Version 2\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from collections import OrderedDict\n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "from functools import reduce\n",
    "import torch.utils.model_zoo as model_zoo\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "from torchvision import datasets\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.dpi'] = 150\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "train_on_gpu = torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading, Preprocessing, Wrangling, Cleansing\n",
    "\n",
    "I chose to functionalize everything in the data preprocessing pipeline for two reasons:\n",
    "\n",
    "- **Reproducibility**  Many times, there is a large body of beautiful code that has been written, but no documentation. The data processing step is usually where people get stuck.\n",
    "- **Iterability** I wanted to iterate fast when chnaging parameters in the module, as well as have one block of code that will take care of everything after restarting the kernal.\n",
    "\n",
    "**Inputs**: \n",
    "- `path_to_train`: Path to your training set folder (I am using PyTorch's `ImageFolder` module)\n",
    "- `path_to_test`: Path to your test set folder\n",
    "- `num_workers`: number of subprocesses to use for data loading\n",
    "- `batch_size`: how many samples per batch to load\n",
    "- `valid_size`: percentage of training set to use as validation\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def DataConstructor2000(path_to_train, path_to_test, classes=None, num_workers=4, batch_size=32, valid_size = 0.2):\n",
    "    \n",
    "  \n",
    "    # Transformations to the image, edit as need be\n",
    "    transform = transforms.Compose([\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.Resize([224,224]),\n",
    "        transforms.ToTensor()])\n",
    "    \n",
    "    train_dataset = datasets.ImageFolder(path_to_train, transform=transform)\n",
    "    print(\"Successfully Loaded Training Set.\")\n",
    "\n",
    "    test_dataset = datasets.ImageFolder(path_to_test, transform=transform)\n",
    "    print(\"Successfully Loaded Test Set.\")\n",
    "\n",
    "    \n",
    "    # obtain training indices that will be used for validation\n",
    "    num_train = len(train_dataset)\n",
    "    indices = list(range(num_train))\n",
    "    np.random.shuffle(indices)\n",
    "    split = int(np.floor(valid_size * num_train))\n",
    "    train_idx, valid_idx = indices[split:], indices[:split]\n",
    "\n",
    "    # define samplers for obtaining training and validation batches\n",
    "    train_sampler = SubsetRandomSampler(train_idx)\n",
    "    valid_sampler = SubsetRandomSampler(valid_idx)\n",
    "\n",
    "    train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, \n",
    "                                               sampler=train_sampler, num_workers=num_workers)\n",
    "\n",
    "    valid_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, \n",
    "                                               sampler=valid_sampler, num_workers=num_workers)\n",
    "\n",
    "    test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=20, \n",
    "        num_workers=num_workers, shuffle=True)\n",
    "    if classes != None:\n",
    "        print(\"Number of Classes:\", len(classes))\n",
    "    return train_loader, valid_loader, test_loader, classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader, valid_loader,test_loader, classes = DataConstructor2000(path_to_train='/workspace/Documents/pretrained-models.pytorch-master/pretrainedmodels/training/',path_to_test='/workspace/Documents/pretrained-models.pytorch-master/pretrainedmodels/test/',\n",
    "                                                                       classes=['Fake','Real'],num_workers=40, batch_size=100, valid_size = 0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize for Confirmation\n",
    "You do not need to change anything here. It should run right out of the box. But feel free to change what you need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "helper function to un-normalize and display an image\n",
    "def imshow(img):\n",
    "#     img = img / 2 + 0.5  # unnormalize if you added normalization in the transformation step\n",
    "    plt.imshow(np.transpose(img, (1, 2, 0)))  # convert from Tensor image\n",
    "\n",
    "# obtain one batch of training images\n",
    "dataiter = iter(train_loader)\n",
    "images, labels = dataiter.next()\n",
    "images = images.numpy() # convert images to numpy for display\n",
    "print(images.shape)\n",
    "\n",
    "# plot the images in the batch, along with the corresponding labels\n",
    "fig = plt.figure(figsize=(25, 4))\n",
    "# display 20 images\n",
    "for idx in np.arange(20):\n",
    "    ax = fig.add_subplot(2, 20/2, idx+1, xticks=[], yticks=[])\n",
    "    imshow(images[idx])\n",
    "    ax.set_title(classes[labels[idx]])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Time to Define the Model\n",
    "\n",
    "So I am using `NASNET` for this implementation. Although I am not getting state of the art performance, I believe a bit of model tuning will get me there. For those interested in the paper, here is the [link](https://arxiv.org/abs/1707.07012). \n",
    "\n",
    "__Abstract.__ The  key  contribution of this work is the design of a new search space (whichwe call the “NASNet search space”) which enables transferability.  In our experiments, we search for the best convolutional layer (or “cell”) on the `CIFAR-10` dataset and then apply this cell to the ImageNet dataset by stacking together more copies of this cell, each with their own parameters to design a convolutional architecture, which we namea “NASNet architecture”.  We also introduce a new regu-larization technique called ScheduledDropPath that significantly improves generalization in the NASNet models.  On `CIFAR-10` itself,  a NASNet found by our method achieves 2.4% error rate, which is state-of-the-art."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class MaxPoolPad(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(MaxPoolPad, self).__init__()\n",
    "        self.pad = nn.ZeroPad2d((1, 0, 1, 0))\n",
    "        self.pool = nn.MaxPool2d(3, stride=2, padding=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pad(x)\n",
    "        x = self.pool(x)\n",
    "        x = x[:, :, 1:, 1:].contiguous()\n",
    "        return x\n",
    "\n",
    "\n",
    "class AvgPoolPad(nn.Module):\n",
    "\n",
    "    def __init__(self, stride=2, padding=1):\n",
    "        super(AvgPoolPad, self).__init__()\n",
    "        self.pad = nn.ZeroPad2d((1, 0, 1, 0))\n",
    "        self.pool = nn.AvgPool2d(3, stride=stride, padding=padding, count_include_pad=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pad(x)\n",
    "        x = self.pool(x)\n",
    "        x = x[:, :, 1:, 1:].contiguous()\n",
    "        return x\n",
    "\n",
    "\n",
    "class SeparableConv2d(nn.Module):\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, dw_kernel, dw_stride, dw_padding, bias=False):\n",
    "        super(SeparableConv2d, self).__init__()\n",
    "        self.depthwise_conv2d = nn.Conv2d(in_channels, in_channels, dw_kernel,\n",
    "                                          stride=dw_stride,\n",
    "                                          padding=dw_padding,\n",
    "                                          bias=bias,\n",
    "                                          groups=in_channels)\n",
    "        self.pointwise_conv2d = nn.Conv2d(in_channels, out_channels, 1, stride=1, bias=bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.depthwise_conv2d(x)\n",
    "        x = self.pointwise_conv2d(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class BranchSeparables(nn.Module):\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, stride, padding, name=None, bias=False):\n",
    "        super(BranchSeparables, self).__init__()\n",
    "        self.relu = nn.ReLU()\n",
    "        self.separable_1 = SeparableConv2d(in_channels, in_channels, kernel_size, stride, padding, bias=bias)\n",
    "        self.bn_sep_1 = nn.BatchNorm2d(in_channels, eps=0.001, momentum=0.1, affine=True)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.separable_2 = SeparableConv2d(in_channels, out_channels, kernel_size, 1, padding, bias=bias)\n",
    "        self.bn_sep_2 = nn.BatchNorm2d(out_channels, eps=0.001, momentum=0.1, affine=True)\n",
    "        self.name = name\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(x)\n",
    "        if self.name == 'specific':\n",
    "            x = nn.ZeroPad2d((1, 0, 1, 0))(x)\n",
    "        x = self.separable_1(x)\n",
    "        if self.name == 'specific':\n",
    "            x = x[:, :, 1:, 1:].contiguous()\n",
    "\n",
    "        x = self.bn_sep_1(x)\n",
    "        x = self.relu1(x)\n",
    "        x = self.separable_2(x)\n",
    "        x = self.bn_sep_2(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class BranchSeparablesStem(nn.Module):\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, stride, padding, bias=False):\n",
    "        super(BranchSeparablesStem, self).__init__()\n",
    "        self.relu = nn.ReLU()\n",
    "        self.separable_1 = SeparableConv2d(in_channels, out_channels, kernel_size, stride, padding, bias=bias)\n",
    "        self.bn_sep_1 = nn.BatchNorm2d(out_channels, eps=0.001, momentum=0.1, affine=True)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.separable_2 = SeparableConv2d(out_channels, out_channels, kernel_size, 1, padding, bias=bias)\n",
    "        self.bn_sep_2 = nn.BatchNorm2d(out_channels, eps=0.001, momentum=0.1, affine=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(x)\n",
    "        x = self.separable_1(x)\n",
    "        x = self.bn_sep_1(x)\n",
    "        x = self.relu1(x)\n",
    "        x = self.separable_2(x)\n",
    "        x = self.bn_sep_2(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class BranchSeparablesReduction(BranchSeparables):\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, stride, padding, z_padding=1, bias=False):\n",
    "        BranchSeparables.__init__(self, in_channels, out_channels, kernel_size, stride, padding, bias)\n",
    "        self.padding = nn.ZeroPad2d((z_padding, 0, z_padding, 0))\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(x)\n",
    "        x = self.padding(x)\n",
    "        x = self.separable_1(x)\n",
    "        x = x[:, :, 1:, 1:].contiguous()\n",
    "        x = self.bn_sep_1(x)\n",
    "        x = self.relu1(x)\n",
    "        x = self.separable_2(x)\n",
    "        x = self.bn_sep_2(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class CellStem0(nn.Module):\n",
    "    def __init__(self, stem_filters, num_filters=42):\n",
    "        super(CellStem0, self).__init__()\n",
    "        self.num_filters = num_filters\n",
    "        self.stem_filters = stem_filters\n",
    "        self.conv_1x1 = nn.Sequential()\n",
    "        self.conv_1x1.add_module('relu', nn.ReLU())\n",
    "        self.conv_1x1.add_module('conv', nn.Conv2d(self.stem_filters, self.num_filters, 1, stride=1, bias=False))\n",
    "        self.conv_1x1.add_module('bn', nn.BatchNorm2d(self.num_filters, eps=0.001, momentum=0.1, affine=True))\n",
    "\n",
    "        self.comb_iter_0_left = BranchSeparables(self.num_filters, self.num_filters, 5, 2, 2)\n",
    "        self.comb_iter_0_right = BranchSeparablesStem(self.stem_filters, self.num_filters, 7, 2, 3, bias=False)\n",
    "\n",
    "        self.comb_iter_1_left = nn.MaxPool2d(3, stride=2, padding=1)\n",
    "        self.comb_iter_1_right = BranchSeparablesStem(self.stem_filters, self.num_filters, 7, 2, 3, bias=False)\n",
    "\n",
    "        self.comb_iter_2_left = nn.AvgPool2d(3, stride=2, padding=1, count_include_pad=False)\n",
    "        self.comb_iter_2_right = BranchSeparablesStem(self.stem_filters, self.num_filters, 5, 2, 2, bias=False)\n",
    "\n",
    "        self.comb_iter_3_right = nn.AvgPool2d(3, stride=1, padding=1, count_include_pad=False)\n",
    "\n",
    "        self.comb_iter_4_left = BranchSeparables(self.num_filters, self.num_filters, 3, 1, 1, bias=False)\n",
    "        self.comb_iter_4_right = nn.MaxPool2d(3, stride=2, padding=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x1 = self.conv_1x1(x)\n",
    "\n",
    "        x_comb_iter_0_left = self.comb_iter_0_left(x1)\n",
    "        x_comb_iter_0_right = self.comb_iter_0_right(x)\n",
    "        x_comb_iter_0 = x_comb_iter_0_left + x_comb_iter_0_right\n",
    "\n",
    "        x_comb_iter_1_left = self.comb_iter_1_left(x1)\n",
    "        x_comb_iter_1_right = self.comb_iter_1_right(x)\n",
    "        x_comb_iter_1 = x_comb_iter_1_left + x_comb_iter_1_right\n",
    "\n",
    "        x_comb_iter_2_left = self.comb_iter_2_left(x1)\n",
    "        x_comb_iter_2_right = self.comb_iter_2_right(x)\n",
    "        x_comb_iter_2 = x_comb_iter_2_left + x_comb_iter_2_right\n",
    "\n",
    "        x_comb_iter_3_right = self.comb_iter_3_right(x_comb_iter_0)\n",
    "        x_comb_iter_3 = x_comb_iter_3_right + x_comb_iter_1\n",
    "\n",
    "        x_comb_iter_4_left = self.comb_iter_4_left(x_comb_iter_0)\n",
    "        x_comb_iter_4_right = self.comb_iter_4_right(x1)\n",
    "        x_comb_iter_4 = x_comb_iter_4_left + x_comb_iter_4_right\n",
    "\n",
    "        x_out = torch.cat([x_comb_iter_1, x_comb_iter_2, x_comb_iter_3, x_comb_iter_4], 1)\n",
    "        return x_out\n",
    "\n",
    "\n",
    "class CellStem1(nn.Module):\n",
    "\n",
    "    def __init__(self, stem_filters, num_filters):\n",
    "        super(CellStem1, self).__init__()\n",
    "        self.num_filters = num_filters\n",
    "        self.stem_filters = stem_filters\n",
    "        self.conv_1x1 = nn.Sequential()\n",
    "        self.conv_1x1.add_module('relu', nn.ReLU())\n",
    "        self.conv_1x1.add_module('conv', nn.Conv2d(2*self.num_filters, self.num_filters, 1, stride=1, bias=False))\n",
    "        self.conv_1x1.add_module('bn', nn.BatchNorm2d(self.num_filters, eps=0.001, momentum=0.1, affine=True))\n",
    "\n",
    "        self.relu = nn.ReLU()\n",
    "        self.path_1 = nn.Sequential()\n",
    "        self.path_1.add_module('avgpool', nn.AvgPool2d(1, stride=2, count_include_pad=False))\n",
    "        self.path_1.add_module('conv', nn.Conv2d(self.stem_filters, self.num_filters//2, 1, stride=1, bias=False))\n",
    "        self.path_2 = nn.ModuleList()\n",
    "        self.path_2.add_module('pad', nn.ZeroPad2d((0, 1, 0, 1)))\n",
    "        self.path_2.add_module('avgpool', nn.AvgPool2d(1, stride=2, count_include_pad=False))\n",
    "        self.path_2.add_module('conv', nn.Conv2d(self.stem_filters, self.num_filters//2, 1, stride=1, bias=False))\n",
    "\n",
    "        self.final_path_bn = nn.BatchNorm2d(self.num_filters, eps=0.001, momentum=0.1, affine=True)\n",
    "\n",
    "        self.comb_iter_0_left = BranchSeparables(self.num_filters, self.num_filters, 5, 2, 2, name='specific', bias=False)\n",
    "        self.comb_iter_0_right = BranchSeparables(self.num_filters, self.num_filters, 7, 2, 3, name='specific', bias=False)\n",
    "\n",
    "        # self.comb_iter_1_left = nn.MaxPool2d(3, stride=2, padding=1)\n",
    "        self.comb_iter_1_left = MaxPoolPad()\n",
    "        self.comb_iter_1_right = BranchSeparables(self.num_filters, self.num_filters, 7, 2, 3, name='specific', bias=False)\n",
    "\n",
    "        # self.comb_iter_2_left = nn.AvgPool2d(3, stride=2, padding=1, count_include_pad=False)\n",
    "        self.comb_iter_2_left = AvgPoolPad()\n",
    "        self.comb_iter_2_right = BranchSeparables(self.num_filters, self.num_filters, 5, 2, 2, name='specific', bias=False)\n",
    "\n",
    "        self.comb_iter_3_right = nn.AvgPool2d(3, stride=1, padding=1, count_include_pad=False)\n",
    "\n",
    "        self.comb_iter_4_left = BranchSeparables(self.num_filters, self.num_filters, 3, 1, 1, name='specific', bias=False)\n",
    "        # self.comb_iter_4_right = nn.MaxPool2d(3, stride=2, padding=1)\n",
    "        self.comb_iter_4_right = MaxPoolPad()\n",
    "\n",
    "    def forward(self, x_conv0, x_stem_0):\n",
    "        x_left = self.conv_1x1(x_stem_0)\n",
    "\n",
    "        x_relu = self.relu(x_conv0)\n",
    "        # path 1\n",
    "        x_path1 = self.path_1(x_relu)\n",
    "        # path 2\n",
    "        x_path2 = self.path_2.pad(x_relu)\n",
    "        x_path2 = x_path2[:, :, 1:, 1:]\n",
    "        x_path2 = self.path_2.avgpool(x_path2)\n",
    "        x_path2 = self.path_2.conv(x_path2)\n",
    "        # final path\n",
    "        x_right = self.final_path_bn(torch.cat([x_path1, x_path2], 1))\n",
    "\n",
    "        x_comb_iter_0_left = self.comb_iter_0_left(x_left)\n",
    "        x_comb_iter_0_right = self.comb_iter_0_right(x_right)\n",
    "        x_comb_iter_0 = x_comb_iter_0_left + x_comb_iter_0_right\n",
    "\n",
    "        x_comb_iter_1_left = self.comb_iter_1_left(x_left)\n",
    "        x_comb_iter_1_right = self.comb_iter_1_right(x_right)\n",
    "        x_comb_iter_1 = x_comb_iter_1_left + x_comb_iter_1_right\n",
    "\n",
    "        x_comb_iter_2_left = self.comb_iter_2_left(x_left)\n",
    "        x_comb_iter_2_right = self.comb_iter_2_right(x_right)\n",
    "        x_comb_iter_2 = x_comb_iter_2_left + x_comb_iter_2_right\n",
    "\n",
    "        x_comb_iter_3_right = self.comb_iter_3_right(x_comb_iter_0)\n",
    "        x_comb_iter_3 = x_comb_iter_3_right + x_comb_iter_1\n",
    "\n",
    "        x_comb_iter_4_left = self.comb_iter_4_left(x_comb_iter_0)\n",
    "        x_comb_iter_4_right = self.comb_iter_4_right(x_left)\n",
    "        x_comb_iter_4 = x_comb_iter_4_left + x_comb_iter_4_right\n",
    "\n",
    "        x_out = torch.cat([x_comb_iter_1, x_comb_iter_2, x_comb_iter_3, x_comb_iter_4], 1)\n",
    "        return x_out\n",
    "\n",
    "\n",
    "class FirstCell(nn.Module):\n",
    "\n",
    "    def __init__(self, in_channels_left, out_channels_left, in_channels_right, out_channels_right):\n",
    "        super(FirstCell, self).__init__()\n",
    "        self.conv_1x1 = nn.Sequential()\n",
    "        self.conv_1x1.add_module('relu', nn.ReLU())\n",
    "        self.conv_1x1.add_module('conv', nn.Conv2d(in_channels_right, out_channels_right, 1, stride=1, bias=False))\n",
    "        self.conv_1x1.add_module('bn', nn.BatchNorm2d(out_channels_right, eps=0.001, momentum=0.1, affine=True))\n",
    "\n",
    "        self.relu = nn.ReLU()\n",
    "        self.path_1 = nn.Sequential()\n",
    "        self.path_1.add_module('avgpool', nn.AvgPool2d(1, stride=2, count_include_pad=False))\n",
    "        self.path_1.add_module('conv', nn.Conv2d(in_channels_left, out_channels_left, 1, stride=1, bias=False))\n",
    "        self.path_2 = nn.ModuleList()\n",
    "        self.path_2.add_module('pad', nn.ZeroPad2d((0, 1, 0, 1)))\n",
    "        self.path_2.add_module('avgpool', nn.AvgPool2d(1, stride=2, count_include_pad=False))\n",
    "        self.path_2.add_module('conv', nn.Conv2d(in_channels_left, out_channels_left, 1, stride=1, bias=False))\n",
    "\n",
    "        self.final_path_bn = nn.BatchNorm2d(out_channels_left * 2, eps=0.001, momentum=0.1, affine=True)\n",
    "\n",
    "        self.comb_iter_0_left = BranchSeparables(out_channels_right, out_channels_right, 5, 1, 2, bias=False)\n",
    "        self.comb_iter_0_right = BranchSeparables(out_channels_right, out_channels_right, 3, 1, 1, bias=False)\n",
    "\n",
    "        self.comb_iter_1_left = BranchSeparables(out_channels_right, out_channels_right, 5, 1, 2, bias=False)\n",
    "        self.comb_iter_1_right = BranchSeparables(out_channels_right, out_channels_right, 3, 1, 1, bias=False)\n",
    "\n",
    "        self.comb_iter_2_left = nn.AvgPool2d(3, stride=1, padding=1, count_include_pad=False)\n",
    "\n",
    "        self.comb_iter_3_left = nn.AvgPool2d(3, stride=1, padding=1, count_include_pad=False)\n",
    "        self.comb_iter_3_right = nn.AvgPool2d(3, stride=1, padding=1, count_include_pad=False)\n",
    "\n",
    "        self.comb_iter_4_left = BranchSeparables(out_channels_right, out_channels_right, 3, 1, 1, bias=False)\n",
    "\n",
    "    def forward(self, x, x_prev):\n",
    "        x_relu = self.relu(x_prev)\n",
    "        # path 1\n",
    "        x_path1 = self.path_1(x_relu)\n",
    "        # path 2\n",
    "        x_path2 = self.path_2.pad(x_relu)\n",
    "        x_path2 = x_path2[:, :, 1:, 1:]\n",
    "        x_path2 = self.path_2.avgpool(x_path2)\n",
    "        x_path2 = self.path_2.conv(x_path2)\n",
    "        # final path\n",
    "        x_left = self.final_path_bn(torch.cat([x_path1, x_path2], 1))\n",
    "\n",
    "        x_right = self.conv_1x1(x)\n",
    "\n",
    "        x_comb_iter_0_left = self.comb_iter_0_left(x_right)\n",
    "        x_comb_iter_0_right = self.comb_iter_0_right(x_left)\n",
    "        x_comb_iter_0 = x_comb_iter_0_left + x_comb_iter_0_right\n",
    "\n",
    "        x_comb_iter_1_left = self.comb_iter_1_left(x_left)\n",
    "        x_comb_iter_1_right = self.comb_iter_1_right(x_left)\n",
    "        x_comb_iter_1 = x_comb_iter_1_left + x_comb_iter_1_right\n",
    "\n",
    "        x_comb_iter_2_left = self.comb_iter_2_left(x_right)\n",
    "        x_comb_iter_2 = x_comb_iter_2_left + x_left\n",
    "\n",
    "        x_comb_iter_3_left = self.comb_iter_3_left(x_left)\n",
    "        x_comb_iter_3_right = self.comb_iter_3_right(x_left)\n",
    "        x_comb_iter_3 = x_comb_iter_3_left + x_comb_iter_3_right\n",
    "\n",
    "        x_comb_iter_4_left = self.comb_iter_4_left(x_right)\n",
    "        x_comb_iter_4 = x_comb_iter_4_left + x_right\n",
    "\n",
    "        x_out = torch.cat([x_left, x_comb_iter_0, x_comb_iter_1, x_comb_iter_2, x_comb_iter_3, x_comb_iter_4], 1)\n",
    "        return x_out\n",
    "\n",
    "\n",
    "class NormalCell(nn.Module):\n",
    "\n",
    "    def __init__(self, in_channels_left, out_channels_left, in_channels_right, out_channels_right):\n",
    "        super(NormalCell, self).__init__()\n",
    "        self.conv_prev_1x1 = nn.Sequential()\n",
    "        self.conv_prev_1x1.add_module('relu', nn.ReLU())\n",
    "        self.conv_prev_1x1.add_module('conv', nn.Conv2d(in_channels_left, out_channels_left, 1, stride=1, bias=False))\n",
    "        self.conv_prev_1x1.add_module('bn', nn.BatchNorm2d(out_channels_left, eps=0.001, momentum=0.1, affine=True))\n",
    "\n",
    "        self.conv_1x1 = nn.Sequential()\n",
    "        self.conv_1x1.add_module('relu', nn.ReLU())\n",
    "        self.conv_1x1.add_module('conv', nn.Conv2d(in_channels_right, out_channels_right, 1, stride=1, bias=False))\n",
    "        self.conv_1x1.add_module('bn', nn.BatchNorm2d(out_channels_right, eps=0.001, momentum=0.1, affine=True))\n",
    "\n",
    "        self.comb_iter_0_left = BranchSeparables(out_channels_right, out_channels_right, 5, 1, 2, bias=False)\n",
    "        self.comb_iter_0_right = BranchSeparables(out_channels_left, out_channels_left, 3, 1, 1, bias=False)\n",
    "\n",
    "        self.comb_iter_1_left = BranchSeparables(out_channels_left, out_channels_left, 5, 1, 2, bias=False)\n",
    "        self.comb_iter_1_right = BranchSeparables(out_channels_left, out_channels_left, 3, 1, 1, bias=False)\n",
    "\n",
    "        self.comb_iter_2_left = nn.AvgPool2d(3, stride=1, padding=1, count_include_pad=False)\n",
    "\n",
    "        self.comb_iter_3_left = nn.AvgPool2d(3, stride=1, padding=1, count_include_pad=False)\n",
    "        self.comb_iter_3_right = nn.AvgPool2d(3, stride=1, padding=1, count_include_pad=False)\n",
    "\n",
    "        self.comb_iter_4_left = BranchSeparables(out_channels_right, out_channels_right, 3, 1, 1, bias=False)\n",
    "\n",
    "    def forward(self, x, x_prev):\n",
    "        x_left = self.conv_prev_1x1(x_prev)\n",
    "        x_right = self.conv_1x1(x)\n",
    "\n",
    "        x_comb_iter_0_left = self.comb_iter_0_left(x_right)\n",
    "        x_comb_iter_0_right = self.comb_iter_0_right(x_left)\n",
    "        x_comb_iter_0 = x_comb_iter_0_left + x_comb_iter_0_right\n",
    "\n",
    "        x_comb_iter_1_left = self.comb_iter_1_left(x_left)\n",
    "        x_comb_iter_1_right = self.comb_iter_1_right(x_left)\n",
    "        x_comb_iter_1 = x_comb_iter_1_left + x_comb_iter_1_right\n",
    "\n",
    "        x_comb_iter_2_left = self.comb_iter_2_left(x_right)\n",
    "        x_comb_iter_2 = x_comb_iter_2_left + x_left\n",
    "\n",
    "        x_comb_iter_3_left = self.comb_iter_3_left(x_left)\n",
    "        x_comb_iter_3_right = self.comb_iter_3_right(x_left)\n",
    "        x_comb_iter_3 = x_comb_iter_3_left + x_comb_iter_3_right\n",
    "\n",
    "        x_comb_iter_4_left = self.comb_iter_4_left(x_right)\n",
    "        x_comb_iter_4 = x_comb_iter_4_left + x_right\n",
    "\n",
    "        x_out = torch.cat([x_left, x_comb_iter_0, x_comb_iter_1, x_comb_iter_2, x_comb_iter_3, x_comb_iter_4], 1)\n",
    "        return x_out\n",
    "\n",
    "\n",
    "class ReductionCell0(nn.Module):\n",
    "\n",
    "    def __init__(self, in_channels_left, out_channels_left, in_channels_right, out_channels_right):\n",
    "        super(ReductionCell0, self).__init__()\n",
    "        self.conv_prev_1x1 = nn.Sequential()\n",
    "        self.conv_prev_1x1.add_module('relu', nn.ReLU())\n",
    "        self.conv_prev_1x1.add_module('conv', nn.Conv2d(in_channels_left, out_channels_left, 1, stride=1, bias=False))\n",
    "        self.conv_prev_1x1.add_module('bn', nn.BatchNorm2d(out_channels_left, eps=0.001, momentum=0.1, affine=True))\n",
    "\n",
    "        self.conv_1x1 = nn.Sequential()\n",
    "        self.conv_1x1.add_module('relu', nn.ReLU())\n",
    "        self.conv_1x1.add_module('conv', nn.Conv2d(in_channels_right, out_channels_right, 1, stride=1, bias=False))\n",
    "        self.conv_1x1.add_module('bn', nn.BatchNorm2d(out_channels_right, eps=0.001, momentum=0.1, affine=True))\n",
    "\n",
    "        self.comb_iter_0_left = BranchSeparablesReduction(out_channels_right, out_channels_right, 5, 2, 2, bias=False)\n",
    "        self.comb_iter_0_right = BranchSeparablesReduction(out_channels_right, out_channels_right, 7, 2, 3, bias=False)\n",
    "\n",
    "        self.comb_iter_1_left = MaxPoolPad()\n",
    "        self.comb_iter_1_right = BranchSeparablesReduction(out_channels_right, out_channels_right, 7, 2, 3, bias=False)\n",
    "\n",
    "        self.comb_iter_2_left = AvgPoolPad()\n",
    "        self.comb_iter_2_right = BranchSeparablesReduction(out_channels_right, out_channels_right, 5, 2, 2, bias=False)\n",
    "\n",
    "        self.comb_iter_3_right = nn.AvgPool2d(3, stride=1, padding=1, count_include_pad=False)\n",
    "\n",
    "        self.comb_iter_4_left = BranchSeparablesReduction(out_channels_right, out_channels_right, 3, 1, 1, bias=False)\n",
    "        self.comb_iter_4_right = MaxPoolPad()\n",
    "\n",
    "    def forward(self, x, x_prev):\n",
    "        x_left = self.conv_prev_1x1(x_prev)\n",
    "        x_right = self.conv_1x1(x)\n",
    "\n",
    "        x_comb_iter_0_left = self.comb_iter_0_left(x_right)\n",
    "        x_comb_iter_0_right = self.comb_iter_0_right(x_left)\n",
    "        x_comb_iter_0 = x_comb_iter_0_left + x_comb_iter_0_right\n",
    "\n",
    "        x_comb_iter_1_left = self.comb_iter_1_left(x_right)\n",
    "        x_comb_iter_1_right = self.comb_iter_1_right(x_left)\n",
    "        x_comb_iter_1 = x_comb_iter_1_left + x_comb_iter_1_right\n",
    "\n",
    "        x_comb_iter_2_left = self.comb_iter_2_left(x_right)\n",
    "        x_comb_iter_2_right = self.comb_iter_2_right(x_left)\n",
    "        x_comb_iter_2 = x_comb_iter_2_left + x_comb_iter_2_right\n",
    "\n",
    "        x_comb_iter_3_right = self.comb_iter_3_right(x_comb_iter_0)\n",
    "        x_comb_iter_3 = x_comb_iter_3_right + x_comb_iter_1\n",
    "\n",
    "        x_comb_iter_4_left = self.comb_iter_4_left(x_comb_iter_0)\n",
    "        x_comb_iter_4_right = self.comb_iter_4_right(x_right)\n",
    "        x_comb_iter_4 = x_comb_iter_4_left + x_comb_iter_4_right\n",
    "\n",
    "        x_out = torch.cat([x_comb_iter_1, x_comb_iter_2, x_comb_iter_3, x_comb_iter_4], 1)\n",
    "        return x_out\n",
    "\n",
    "\n",
    "class ReductionCell1(nn.Module):\n",
    "\n",
    "    def __init__(self, in_channels_left, out_channels_left, in_channels_right, out_channels_right):\n",
    "        super(ReductionCell1, self).__init__()\n",
    "        self.conv_prev_1x1 = nn.Sequential()\n",
    "        self.conv_prev_1x1.add_module('relu', nn.ReLU())\n",
    "        self.conv_prev_1x1.add_module('conv', nn.Conv2d(in_channels_left, out_channels_left, 1, stride=1, bias=False))\n",
    "        self.conv_prev_1x1.add_module('bn', nn.BatchNorm2d(out_channels_left, eps=0.001, momentum=0.1, affine=True))\n",
    "\n",
    "        self.conv_1x1 = nn.Sequential()\n",
    "        self.conv_1x1.add_module('relu', nn.ReLU())\n",
    "        self.conv_1x1.add_module('conv', nn.Conv2d(in_channels_right, out_channels_right, 1, stride=1, bias=False))\n",
    "        self.conv_1x1.add_module('bn', nn.BatchNorm2d(out_channels_right, eps=0.001, momentum=0.1, affine=True))\n",
    "\n",
    "        self.comb_iter_0_left = BranchSeparables(out_channels_right, out_channels_right, 5, 2, 2, name='specific', bias=False)\n",
    "        self.comb_iter_0_right = BranchSeparables(out_channels_right, out_channels_right, 7, 2, 3, name='specific', bias=False)\n",
    "\n",
    "        # self.comb_iter_1_left = nn.MaxPool2d(3, stride=2, padding=1)\n",
    "        self.comb_iter_1_left = MaxPoolPad()\n",
    "        self.comb_iter_1_right = BranchSeparables(out_channels_right, out_channels_right, 7, 2, 3, name='specific', bias=False)\n",
    "\n",
    "        # self.comb_iter_2_left = nn.AvgPool2d(3, stride=2, padding=1, count_include_pad=False)\n",
    "        self.comb_iter_2_left = AvgPoolPad()\n",
    "        self.comb_iter_2_right = BranchSeparables(out_channels_right, out_channels_right, 5, 2, 2, name='specific', bias=False)\n",
    "\n",
    "        self.comb_iter_3_right = nn.AvgPool2d(3, stride=1, padding=1, count_include_pad=False)\n",
    "\n",
    "        self.comb_iter_4_left = BranchSeparables(out_channels_right, out_channels_right, 3, 1, 1, name='specific', bias=False)\n",
    "        # self.comb_iter_4_right = nn.MaxPool2d(3, stride=2, padding=1)\n",
    "        self.comb_iter_4_right =MaxPoolPad()\n",
    "\n",
    "    def forward(self, x, x_prev):\n",
    "        x_left = self.conv_prev_1x1(x_prev)\n",
    "        x_right = self.conv_1x1(x)\n",
    "\n",
    "        x_comb_iter_0_left = self.comb_iter_0_left(x_right)\n",
    "        x_comb_iter_0_right = self.comb_iter_0_right(x_left)\n",
    "        x_comb_iter_0 = x_comb_iter_0_left + x_comb_iter_0_right\n",
    "\n",
    "        x_comb_iter_1_left = self.comb_iter_1_left(x_right)\n",
    "        x_comb_iter_1_right = self.comb_iter_1_right(x_left)\n",
    "        x_comb_iter_1 = x_comb_iter_1_left + x_comb_iter_1_right\n",
    "\n",
    "        x_comb_iter_2_left = self.comb_iter_2_left(x_right)\n",
    "        x_comb_iter_2_right = self.comb_iter_2_right(x_left)\n",
    "        x_comb_iter_2 = x_comb_iter_2_left + x_comb_iter_2_right\n",
    "\n",
    "        x_comb_iter_3_right = self.comb_iter_3_right(x_comb_iter_0)\n",
    "        x_comb_iter_3 = x_comb_iter_3_right + x_comb_iter_1\n",
    "\n",
    "        x_comb_iter_4_left = self.comb_iter_4_left(x_comb_iter_0)\n",
    "        x_comb_iter_4_right = self.comb_iter_4_right(x_right)\n",
    "        x_comb_iter_4 = x_comb_iter_4_left + x_comb_iter_4_right\n",
    "\n",
    "        x_out = torch.cat([x_comb_iter_1, x_comb_iter_2, x_comb_iter_3, x_comb_iter_4], 1)\n",
    "        return x_out\n",
    "\n",
    "\n",
    "class NASNetAMobile(nn.Module):\n",
    "    \"\"\"NASNetAMobile (4 @ 1056) \"\"\"\n",
    "\n",
    "    def __init__(self, num_classes=1001, stem_filters=32, penultimate_filters=1056, filters_multiplier=2):\n",
    "        super(NASNetAMobile, self).__init__()\n",
    "        self.num_classes = num_classes\n",
    "        self.stem_filters = stem_filters\n",
    "        self.penultimate_filters = penultimate_filters\n",
    "        self.filters_multiplier = filters_multiplier\n",
    "\n",
    "        filters = self.penultimate_filters // 24\n",
    "        # 24 is default value for the architecture\n",
    "\n",
    "        self.conv0 = nn.Sequential()\n",
    "        self.conv0.add_module('conv', nn.Conv2d(in_channels=3, out_channels=self.stem_filters, kernel_size=3, padding=0, stride=2,\n",
    "                                                bias=False))\n",
    "        self.conv0.add_module('bn', nn.BatchNorm2d(self.stem_filters, eps=0.001, momentum=0.1, affine=True))\n",
    "\n",
    "        self.cell_stem_0 = CellStem0(self.stem_filters, num_filters=filters // (filters_multiplier ** 2))\n",
    "        self.cell_stem_1 = CellStem1(self.stem_filters, num_filters=filters // filters_multiplier)\n",
    "\n",
    "        self.cell_0 = FirstCell(in_channels_left=filters, out_channels_left=filters//2, # 1, 0.5\n",
    "                                in_channels_right=2*filters, out_channels_right=filters) # 2, 1\n",
    "        self.cell_1 = NormalCell(in_channels_left=2*filters, out_channels_left=filters, # 2, 1\n",
    "                                 in_channels_right=6*filters, out_channels_right=filters) # 6, 1\n",
    "        self.cell_2 = NormalCell(in_channels_left=6*filters, out_channels_left=filters, # 6, 1\n",
    "                                 in_channels_right=6*filters, out_channels_right=filters) # 6, 1\n",
    "        self.cell_3 = NormalCell(in_channels_left=6*filters, out_channels_left=filters, # 6, 1\n",
    "                                 in_channels_right=6*filters, out_channels_right=filters) # 6, 1\n",
    "\n",
    "        self.reduction_cell_0 = ReductionCell0(in_channels_left=6*filters, out_channels_left=2*filters, # 6, 2\n",
    "                                               in_channels_right=6*filters, out_channels_right=2*filters) # 6, 2\n",
    "\n",
    "        self.cell_6 = FirstCell(in_channels_left=6*filters, out_channels_left=filters, # 6, 1\n",
    "                                in_channels_right=8*filters, out_channels_right=2*filters) # 8, 2\n",
    "        self.cell_7 = NormalCell(in_channels_left=8*filters, out_channels_left=2*filters, # 8, 2\n",
    "                                 in_channels_right=12*filters, out_channels_right=2*filters) # 12, 2\n",
    "        self.cell_8 = NormalCell(in_channels_left=12*filters, out_channels_left=2*filters, # 12, 2\n",
    "                                 in_channels_right=12*filters, out_channels_right=2*filters) # 12, 2\n",
    "        self.cell_9 = NormalCell(in_channels_left=12*filters, out_channels_left=2*filters, # 12, 2\n",
    "                                 in_channels_right=12*filters, out_channels_right=2*filters) # 12, 2\n",
    "\n",
    "        self.reduction_cell_1 = ReductionCell1(in_channels_left=12*filters, out_channels_left=4*filters, # 12, 4\n",
    "                                               in_channels_right=12*filters, out_channels_right=4*filters) # 12, 4\n",
    "\n",
    "        self.cell_12 = FirstCell(in_channels_left=12*filters, out_channels_left=2*filters, # 12, 2\n",
    "                                 in_channels_right=16*filters, out_channels_right=4*filters) # 16, 4\n",
    "        self.cell_13 = NormalCell(in_channels_left=16*filters, out_channels_left=4*filters, # 16, 4\n",
    "                                  in_channels_right=24*filters, out_channels_right=4*filters) # 24, 4\n",
    "        self.cell_14 = NormalCell(in_channels_left=24*filters, out_channels_left=4*filters, # 24, 4\n",
    "                                  in_channels_right=24*filters, out_channels_right=4*filters) # 24, 4\n",
    "        self.cell_15 = NormalCell(in_channels_left=24*filters, out_channels_left=4*filters, # 24, 4\n",
    "                                  in_channels_right=24*filters, out_channels_right=4*filters) # 24, 4\n",
    "\n",
    "        self.relu = nn.ReLU()\n",
    "        self.avg_pool = nn.AvgPool2d(7, stride=1, padding=0)\n",
    "        self.dropout = nn.Dropout()\n",
    "        self.last_linear = nn.Linear(24*filters, self.num_classes)\n",
    "\n",
    "    def features(self, input):\n",
    "        x_conv0 = self.conv0(input)\n",
    "        x_stem_0 = self.cell_stem_0(x_conv0)\n",
    "        x_stem_1 = self.cell_stem_1(x_conv0, x_stem_0)\n",
    "\n",
    "        x_cell_0 = self.cell_0(x_stem_1, x_stem_0)\n",
    "        x_cell_1 = self.cell_1(x_cell_0, x_stem_1)\n",
    "        x_cell_2 = self.cell_2(x_cell_1, x_cell_0)\n",
    "        x_cell_3 = self.cell_3(x_cell_2, x_cell_1)\n",
    "\n",
    "        x_reduction_cell_0 = self.reduction_cell_0(x_cell_3, x_cell_2)\n",
    "\n",
    "        x_cell_6 = self.cell_6(x_reduction_cell_0, x_cell_3)\n",
    "        x_cell_7 = self.cell_7(x_cell_6, x_reduction_cell_0)\n",
    "        x_cell_8 = self.cell_8(x_cell_7, x_cell_6)\n",
    "        x_cell_9 = self.cell_9(x_cell_8, x_cell_7)\n",
    "\n",
    "        x_reduction_cell_1 = self.reduction_cell_1(x_cell_9, x_cell_8)\n",
    "\n",
    "        x_cell_12 = self.cell_12(x_reduction_cell_1, x_cell_9)\n",
    "        x_cell_13 = self.cell_13(x_cell_12, x_reduction_cell_1)\n",
    "        x_cell_14 = self.cell_14(x_cell_13, x_cell_12)\n",
    "        x_cell_15 = self.cell_15(x_cell_14, x_cell_13)\n",
    "        return x_cell_15\n",
    "\n",
    "    def logits(self, features):\n",
    "        x = self.relu(features)\n",
    "        x = self.avg_pool(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.dropout(x)\n",
    "        x = self.last_linear(x)\n",
    "        return x\n",
    "\n",
    "    def forward(self, input):\n",
    "        x = self.features(input)\n",
    "        x = self.logits(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "def nasnetamobile(num_classes=2):\n",
    "    model = NASNetAMobile(num_classes=num_classes)\n",
    "    return model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = nasnetamobile(num_classes=2)\n",
    "model = nn.DataParallel(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Choosing a Loss Function and Optimizer\n",
    "I went with Cross-Entropy Loss. __Cross-entropy loss__, or log loss, measures the performance of a classification model whose output is a probability value between 0 and 1. Cross-entropy loss increases as the predicted probability diverges from the actual label. Hence, predicting a probability of .012 when the actual observation label is 1 would be bad and result in a high loss value. A perfect model would have a log loss of 0.\n",
    "    $$\\text{loss}(x, class) = -\\log\\left(\\frac{\\exp(x[class])}{\\sum_j \\exp(x[j])}\\right)\n",
    "                   = -x[class] + \\log\\left(\\sum_j \\exp(x[j])\\right)$$\n",
    "                   \n",
    "I opted with good old __Stochastic Gradient Descent__. Nuff said."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify loss function (categorical cross-entropy)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# specify optimizer/secret sauce\n",
    "optimizer = optimizer = torch.optim.SGD([\n",
    "    {'params': list(model.parameters())[:-1], 'lr': 1e-3, 'momentum': 0.9, 'weight_decay': 1e-3},\n",
    "    {'params': list(model.parameters())[-1], 'lr': 5e-5, 'momentum': 0.9, 'weight_decay': 1e-5}\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Time to Train\n",
    "This is where stripes are earned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of epochs to train the model\n",
    "n_epochs = 30\n",
    "\n",
    "valid_loss_min = np.Inf # track change in validation loss\n",
    "training_vis = []\n",
    "valid_vis = []\n",
    "\n",
    "for epoch in range(1, n_epochs+1):\n",
    "\n",
    "    # keep track of training and validation loss\n",
    "    train_loss = 0.0\n",
    "    valid_loss = 0.0\n",
    "    \n",
    "    ###################\n",
    "    # train the model #\n",
    "    ###################\n",
    "    model.train()\n",
    "    for data, target in train_loader:\n",
    "        # move tensors to GPU if CUDA is available\n",
    "        if train_on_gpu:\n",
    "            data, target = data.cuda(), target.cuda()\n",
    "        \n",
    "        # clear the gradients of all optimized variables\n",
    "        optimizer.zero_grad()\n",
    "        # forward pass: compute predicted outputs by passing inputs to the model\n",
    "        output = model(data)\n",
    "        # calculate the batch loss\n",
    "        loss = criterion(output, target)\n",
    "        # backward pass: compute gradient of the loss with respect to model parameters\n",
    "        loss.backward()\n",
    "        # perform a single optimization step (parameter update)\n",
    "        optimizer.step()\n",
    "        # update training loss\n",
    "        train_loss += loss.item()*data.size(0)\n",
    "\n",
    "        \n",
    "    ######################    \n",
    "    # validate the model #\n",
    "    ######################\n",
    "    model.eval()\n",
    "    for data, target in valid_loader:\n",
    "        # move tensors to GPU if CUDA is available\n",
    "        if train_on_gpu:\n",
    "            data, target = data.cuda(), target.cuda()\n",
    "        \n",
    "        # forward pass: compute predicted outputs by passing inputs to the model\n",
    "        output = model(data)\n",
    "        # calculate the batch loss\n",
    "        loss = criterion(output, target)\n",
    "        # update average validation loss \n",
    "        valid_loss += loss.item()*data.size(0)\n",
    "        \n",
    "    # calculate average losses\n",
    "    train_loss = train_loss/len(train_loader.dataset)\n",
    "    valid_loss = valid_loss/len(valid_loader.dataset)\n",
    "    \n",
    "    training_vis.append(train_loss)\n",
    "    valid_vis.append(valid_loss)\n",
    "    \n",
    "    # print training/validation statistics \n",
    "    print('Epoch: {} \\tTraining Loss: {:.6f} \\tValidation Loss: {:.6f}'.format(\n",
    "        epoch, train_loss, valid_loss))\n",
    "    \n",
    "    # save model if validation loss has decreased\n",
    "    if valid_loss <= valid_loss_min:\n",
    "        print('\\nValidation loss decreased ({:.6f} --> {:.6f}).  Saving model ...'.format(\n",
    "        valid_loss_min,\n",
    "        valid_loss))\n",
    "        torch.save(model.state_dict(), 'NasnetMobile35656_gan-detector_Final.pt')\n",
    "        valid_loss_min = valid_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the Saved Model\n",
    "\n",
    "Since we are smart enough to save the best validation error, we can easily load the model and make our predictions on our test set if need be."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load('NasnetMobile35656_gan-detector_Final.pt'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How Well Can we Generalize?\n",
    "\n",
    "Here, we will compute the test error and test accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# track test loss\n",
    "test_loss = 0.0\n",
    "class_correct = list(0. for i in range(2))\n",
    "class_total = list(0. for i in range(2))\n",
    "test_vis = []\n",
    "model.eval()\n",
    "# iterate over test data\n",
    "for data, target in valid_loader:\n",
    "#     print(target)\n",
    "    # move tensors to GPU if CUDA is available\n",
    "    if train_on_gpu:\n",
    "        data, target = data.cuda(), target.cuda()\n",
    "    # forward pass: compute predicted outputs by passing inputs to the model\n",
    "    output = model(data)\n",
    "#     print(output)\n",
    "    # calculate the batch loss\n",
    "    loss = criterion(output, target)\n",
    "    # update test loss \n",
    "    test_loss += loss.item()*data.size(0)\n",
    "    \n",
    "    # convert output probabilities to predicted class\n",
    "    _, pred = torch.max(output, 1)\n",
    "#     print(pred)\n",
    "    # compare predictions to true label\n",
    "    correct_tensor = pred.eq(target.data.view_as(pred))\n",
    "    correct = np.squeeze(correct_tensor.numpy()) if not train_on_gpu else np.squeeze(correct_tensor.cpu().numpy())\n",
    "    # calculate test accuracy for each object class\n",
    "    for i in range(2):\n",
    "#         print(i)\n",
    "        label = target.data[i]\n",
    "        class_correct[label] += correct[i].item()\n",
    "        class_total[label] += 1\n",
    "\n",
    "# average test loss\n",
    "test_loss = test_loss/len(test_loader.dataset)\n",
    "print('Test Loss: {:.6f}\\n'.format(test_loss))\n",
    "test_vis.append(test_loss)\n",
    "for i in range(2):\n",
    "    if class_total[i] > 0:\n",
    "        print('Test Accuracy of %5s: %2d%% (%2d/%2d)' % (\n",
    "            classes[i], 100 * class_correct[i] / class_total[i],\n",
    "            np.sum(class_correct[i]), np.sum(class_total[i])))\n",
    "    else:\n",
    "        print('Test Accuracy of %5s: N/A (no training examples)' % (classes[i]))\n",
    "\n",
    "print('\\nTest Accuracy (Overall): %2d%% (%2d/%2d)' % (\n",
    "    100. * np.sum(class_correct) / np.sum(class_total),\n",
    "    np.sum(class_correct), np.sum(class_total)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize the Training and Validation Errors\n",
    "\n",
    "To see how our model is doing as it runs through different epochs, we need to visualize the training. I tried to make it pretty, but honestly this works for now. Orange is the __Validation__ and Blue is the __Training__."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(range(epoch), training_vis)\n",
    "# plt.scatter(range(epoch), training_vis)\n",
    "plt.plot(range(epoch), valid_vis)\n",
    "# plt.scatter(range(epoch), valid_vis)\n",
    "plt.title('Training vs. Validation')\n",
    "plt.xlabel('Number of Epochs')\n",
    "plt.ylabel('Error')\n",
    "plt.savefig('Nasnet_line.svg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt('NasNet_Final.txt', np.array([training_vis, valid_vis]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Which images did we get Wrong and Right?\n",
    "\n",
    "I could have made this a function, but it works. \n",
    "\n",
    "Format: __Predicted (Truth)__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# obtain one batch of test images\n",
    "dataiter = iter(test_loader)\n",
    "images, labels = dataiter.next()\n",
    "\n",
    "# get sample outputs\n",
    "output = model(images)\n",
    "# convert output probabilities to predicted class\n",
    "_, preds = torch.max(output, 1)\n",
    "# prep images for display\n",
    "images = images.numpy()\n",
    "labels = labels.numpy()\n",
    "print(images.shape)\n",
    "# plot the images in the batch, along with predicted and true labels\n",
    "fig = plt.figure(figsize=(20, 4))\n",
    "for idx in np.arange(20):\n",
    "    ax = fig.add_subplot(2, 20/2, idx+1, xticks=[], yticks=[])\n",
    "    ax.imshow(np.swapaxes((images[idx]),axis1=0, axis2=2))\n",
    "    ax.set_title(\"{} ({})\".format(classes[preds[idx]], classes[labels[idx]]),\n",
    "                 color=(\"green\" if classes[preds[idx]]==classes[labels[idx]] else \"red\"))\n",
    "plt.savefig('PNASNET_mis.pdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploratory Analysis/Sanity Check\n",
    "\n",
    "Basically just plots the corresponding value with the color."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rgb_img = np.squeeze(images[3])\n",
    "channels = ['red channel', 'green channel', 'blue channel']\n",
    "\n",
    "fig = plt.figure(figsize = (36, 36)) \n",
    "for idx in np.arange(rgb_img.shape[0]):\n",
    "    ax = fig.add_subplot(1, 3, idx + 1)\n",
    "    img = rgb_img[idx]\n",
    "    ax.imshow(img, cmap='gray')\n",
    "    ax.set_title(channels[idx])\n",
    "    width, height = img.shape\n",
    "    thresh = img.max()/2.5\n",
    "    for x in range(width):\n",
    "        for y in range(height):\n",
    "            val = round(img[x][y],2) if img[x][y] !=0 else 0\n",
    "            ax.annotate(str(val), xy=(y,x),\n",
    "                    horizontalalignment='center',\n",
    "                    verticalalignment='center', size=8,\n",
    "                    color='white' if img[x][y]<thresh else 'black')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
